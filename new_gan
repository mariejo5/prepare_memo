import tensorflow as tf
import numpy as np
import pandas as pd

# Créer un DataFrame simple pour le churn
num_no_churn = 15  # Nombre de clients qui n'ont pas quitté
num_churn = 5      # Nombre de clients qui ont quitté

# Données pour les clients qui n'ont pas quitté
data_no_churn = {
    'Age': np.random.randint(25, 60, size=num_no_churn),
    'Monthly_Spend': np.random.uniform(20.0, 100.0, size=num_no_churn),
    'Satisfaction_Score': np.random.uniform(7.0, 10.0, size=num_no_churn)
}

# Données pour les clients qui ont quitté
data_churn = {
    'Age': np.random.randint(25, 60, size=num_churn),
    'Monthly_Spend': np.random.uniform(20.0, 100.0, size=num_churn),
    'Satisfaction_Score': np.random.uniform(0.0, 6.0, size=num_churn)
}

# Créer des DataFrames pour chaque classe
df_no_churn = pd.DataFrame(data_no_churn)
df_churn = pd.DataFrame(data_churn)

# Ajouter une colonne 'Churn' pour les classes
df_no_churn['Churn'] = 0  # 0 pour clients qui n'ont pas quitté
df_churn['Churn'] = 1      # 1 pour clients qui ont quitté

# Combiner les DataFrames
X_train = pd.concat([df_no_churn, df_churn], ignore_index=True)
y_train = X_train['Churn']

# Créer un DataFrame original sans variables catégorielles
original_df = X_train.copy()

# Afficher les caractéristiques du jeu de données original
print("Jeu de données original :")
print(original_df)

# Convertir les variables en un tableau de caractéristiques
X_train = X_train.drop(columns=['Churn']).astype(np.float32)

# Définir les composants du GAN

NUM_FEATURES = X_train.shape[1]


# Modèle du générateur
def build_generator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(100,)))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(64))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(NUM_FEATURES, activation='tanh'))  # Ajuster pour le nombre de features
    return model

# Modèle du discriminateur
def build_discriminator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(NUM_FEATURES,)))  # Ajuster pour le nombre de features
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(128))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    return model

# Définir les entrées pour les données réelles et le vecteur de bruit
real_data = tf.keras.Input(shape=(NUM_FEATURES,))
z = tf.keras.Input(shape=(100,))

# Construire les modèles
generator = build_generator()
discriminator = build_discriminator()

# Compiler le discriminateur
discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                      loss='binary_crossentropy')

# Pour le générateur, empiler le générateur et le discriminateur
discriminator.trainable = False

gan_input = tf.keras.Input(shape=(100,))
generated_data = generator(gan_input)
gan_output = discriminator(generated_data)
gan = tf.keras.Model(gan_input, gan_output)

gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy')

# Fonction pour créer des lots
def next_batch(num, data):
    idx = np.arange(0, len(data))
    np.random.shuffle(idx)
    data_shuffle = data.iloc[idx]
    return data_shuffle[:num].values

# Extraire uniquement les échantillons de la classe minoritaire (churn)
X_ones = X_train[y_train == 1].reset_index(drop=True)

batch_size = 5
epochs = 8  # Utilisation de seulement 8 époques pour la démonstration
samples = []
no_of_samples_to_generate = num_no_churn - num_churn  # Calculer le nombre d'échantillons à générer pour équilibrer

# Entraîner le GAN
for epoch in range(epochs):
    num_batches = len(X_ones) // batch_size
    for i in range(num_batches):
        batch_X = next_batch(batch_size, X_ones)
        batch_z = np.random.uniform(-1, 1, size=(batch_size, 100))
        
        # Entraîner le discriminateur
        generated_data = generator.predict(batch_z)
        d_loss_real = discriminator.train_on_batch(batch_X, np.ones((batch_size, 1)) * 0.9)
        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))
        d_loss = 0.5 * (d_loss_real + d_loss_fake)
        
        # Entraîner le générateur
        g_loss = gan.train_on_batch(batch_z, np.ones((batch_size, 1)))
    
    print(f"Epoch {epoch+1}/{epochs} completed. D Loss: {d_loss}, G Loss: {g_loss}")

# Générer de nouveaux échantillons pour équilibrer le dataset
sample_z = np.random.uniform(-1, 1, size=(no_of_samples_to_generate, 100))
gen_samples = generator.predict(sample_z)

# Créer un DataFrame à partir des échantillons générés
generated_df = pd.DataFrame(gen_samples, columns=X_train.columns)

# Créer des labels pour les échantillons générés (classe 1)
generated_labels = pd.Series(np.ones(len(generated_df)))

# Concaténer les DataFrames générés avec l'ensemble de données original
balanced_X_train = pd.concat([X_train, generated_df], ignore_index=True)
balanced_y_train = pd.concat([y_train, generated_labels], ignore_index=True)

# Afficher les résultats
balanced_df = pd.DataFrame(balanced_X_train, columns=X_train.columns)
balanced_df['Churn'] = balanced_y_train.values

# Vérifier la forme de l'ensemble de données équilibré
print(f"\nJeu de données équilibré :")
print(balanced_df)

# Afficher la distribution des classes
print("\nDistribution des classes (Churn) :")
print(balanced_y_train.value_counts())
