# Chargement de votre jeu de données (remplacez par le nom de votre fichier)
##data = pd.read_csv('votre_fichier.csv')  # Remplacez 'votre_fichier.csv' par le chemin réel de votre fichier

# Définir la variable cible et les caractéristiques
variable_cible = 'Target'  # Remplacez 'target' par le nom de votre colonne de variable cible
features = df.drop(columns=[variable_cible])  # Supprimer la colonne cible pour obtenir les caractéristiques
target = df[variable_cible]  # Extraire la colonne cible
# Vérifier et convertir les colonnes catégorielles en numériques (le cas échéant)
features = pd.get_dummies(features, drop_first=True)

# Normaliser les données (mise à l'échelle entre -1 et 1)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(-1, 1))
features = scaler.fit_transform(features)

# Redéfinir les DataFrames pour l'entraînement
X_train = pd.DataFrame(features)
y_train = target.reset_index(drop=True)
# Modèle du générateur
def build_generator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(100,)))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(64))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(NUM_FEATURES, activation='tanh'))  # Ajuster pour le nombre de features
    return model

# Modèle du discriminateur
def build_discriminator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(NUM_FEATURES,)))  # Ajuster pour le nombre de features
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(128))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    return model

NUM_FEATURES = X_train.shape[1]

# Modèles du générateur et du discriminateur comme dans le code original
# (Code pour 'build_generator' et 'build_discriminator' reste le même)
# ...

# Extraire uniquement les échantillons de la classe minoritaire
minority_class = 1  # Supposons que la classe minoritaire est '1'
X_ones = X_train[y_train == minority_class].reset_index(drop=True)

# Définir le batch_size, le nombre d'époques, et les échantillons à générer
batch_size = 5
epochs = 8
num_no_churn = (y_train == 0).sum()  # Nombre d'échantillons de la classe majoritaire
num_churn = (y_train == 1).sum()  # Nombre d'échantillons de la classe minoritaire
no_of_samples_to_generate = num_no_churn - num_churn  # Échantillons à générer
# Fonction pour créer des lots de données
def next_batch(num, data):
    # Générer un ensemble d'indices aléatoires
    idx = np.random.randint(0, len(data), size=num)
    # Sélectionner les données correspondantes à ces indices
    return data.iloc[idx]

# Modèle du générateur
def build_generator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(100,)))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(64))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(NUM_FEATURES, activation='tanh'))  # Ajuster pour le nombre de features
    return model

# Modèle du discriminateur
def build_discriminator():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_shape=(NUM_FEATURES,)))  # Ajuster pour le nombre de features
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(128))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.01))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    return model

# Créer les modèles du générateur et du discriminateur
generator = build_generator()
discriminator = build_discriminator()

# Compiler le discriminateur
discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                      loss='binary_crossentropy')

# Empiler le GAN (générateur + discriminateur)
discriminator.trainable = False  # Fixer le discriminateur pour l'entraînement du GAN
gan_input = tf.keras.Input(shape=(100,))
generated_data = generator(gan_input)
gan_output = discriminator(generated_data)
gan = tf.keras.Model(gan_input, gan_output)

# Compiler le GAN
gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy')

# Ensuite, vous pouvez passer à la boucle d'entraînement pour les deux modèles.
import tensorflow as tf
import numpy as np
import pandas as pd
# Entraîner le GAN
for epoch in range(epochs):
    num_batches = len(X_ones) // batch_size
    for i in range(num_batches):
        batch_X = next_batch(batch_size, X_ones)
        batch_z = np.random.uniform(-1, 1, size=(batch_size, 100))
        
        # Entraîner le discriminateur
        generated_data = generator.predict(batch_z)
        d_loss_real = discriminator.train_on_batch(batch_X, np.ones((batch_size, 1)) * 0.9)
        d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))
        d_loss = 0.5 * (d_loss_real + d_loss_fake)
        
        # Entraîner le générateur
        g_loss = gan.train_on_batch(batch_z, np.ones((batch_size, 1)))
    
    print(f"Epoch {epoch+1}/{epochs} completed. D Loss: {d_loss}, G Loss: {g_loss}")

# Générer les nouveaux échantillons
sample_z = np.random.uniform(-1, 1, size=(no_of_samples_to_generate, 100))
gen_samples = generator.predict(sample_z)

# Inverser la normalisation des échantillons générés
gen_samples = scaler.inverse_transform(gen_samples)

# Ajouter les labels pour les nouveaux échantillons
generated_df = pd.DataFrame(gen_samples, columns=X_train.columns)
generated_labels = pd.Series(np.ones(len(generated_df)))

# Rééquilibrer le DataFrame d'entraînement
balanced_X_train = pd.concat([pd.DataFrame(features), generated_df], ignore_index=True)
balanced_y_train = pd.concat([y_train, generated_labels], ignore_index=True)

# Vérifier la distribution des classes rééquilibrées
print("\nDistribution des classes rééquilibrées :")
print(balanced_y_train.value_counts())
